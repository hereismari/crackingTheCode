# Duplicate URLs

## Problem

You have 10 billion URLs how to detect two identical URLs?

## Proposals

### Small scale

Considering that we have only 100 URLs (to keep it simple) we could
come up with simple regex rules to remove things like "http://", "www", ... The problem with this is that is easy to make mistakes like:

www.page_1.com/ can work, but page_1.com doesn't.

So one thing we could do is to hash the content to the URL. And if we
have seen something before we count the duplicates.

### Large scale

10 billion URLs, if each URLS is on average 100 chars, and each char takes
4 bytes. We would need 10 billion * 100 * 4 bytes = 10 * 10^7 * 10^2 * 4 bytes = 4 * 10^10 bytes = 40 giga bytes.

We could split this in 40 chunks of 1 gb.

Use the small scale approach in a distributed way.
it would be O(n) but we can parallelize this in multiple machines.

